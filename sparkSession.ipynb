{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a77873",
   "metadata": {},
   "source": [
    "# Spark Tutorial\n",
    "Hands-on, in-class session to get practical experience with one of the most popular and widely used Data Science platforms.\n",
    "As you run spark commands, try to identify when lazy evaluation is applied/useful.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* Installation\n",
    "* Euler sum, RDDs\n",
    "* Spark DataFrames\n",
    "* Column selection\n",
    "* Filters\n",
    "* Sorting\n",
    "* Joins, lazy evaluation\n",
    "\n",
    "## Installation\n",
    "I suggest to follow https://sparkbyexamples.com/pyspark/install-pyspark-in-anaconda-jupyter-notebook/\n",
    "\n",
    "In essence, do:\n",
    "`conda create -n sparkds python=3.9\n",
    "conda activate sparkds\n",
    "conda install -y openjdk\n",
    "conda install -y pyspark \n",
    "conda install -c conda-forge findspark\n",
    "conda install -y jupyter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce48bf0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ahenschel/Applications/miniconda3/envs/pyspark/lib/python3.9/site-packages/pyspark'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## installed in pyspark environment\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8de7b18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/ahenschel/Applications/miniconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/11/10 17:21:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"COSC608_spark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b5f6870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.10.173.123:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>COSC608_spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fedd567d5e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8dffb681",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For older Spark (2.*) versions, often a context object is used\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3475601a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://macbook-pro-10:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>COSC608_spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff67cefcd60>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efe0280",
   "metadata": {},
   "source": [
    "## First steps\n",
    "Creating the first DataFrame. They are similar, yet with subtle differences wrt Pandas dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3cfc23",
   "metadata": {},
   "source": [
    "## Euler Sum\n",
    "Implement Eulers sum, as per the slides. Use parallelization and experiment with\n",
    "$\\lim_{n->\\infty}\\sum_{i=1}^{n}\\frac{1}{i^2} = \\frac{\\pi^2}{6}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cfac327f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyspark\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "18988063",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000000\n",
    "ar = np.arange(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57c76a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "661e2897",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpartitions = 16\n",
    "dat = sc.parallelize(ar, numpartitions)\n",
    "sqrs = dat.map(lambda i: 1.0/(i+1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "0273430d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sqrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e10486c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/01 13:42:26 WARN TaskSetManager: Stage 8 contains a task of very large size (11661 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "x = sqrs.reduce(lambda a,b: a+b)\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a9f29c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 1.644934\n",
      "time=4.974174\n"
     ]
    }
   ],
   "source": [
    "print(\"x = %f\"%x)\n",
    "print(\"time=%f\"%(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93aa2764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 1.644934\n",
      "time=3.844444\n"
     ]
    }
   ],
   "source": [
    "x = 0.0\n",
    "t0 = time.time()\n",
    "for i in range(1,n):\n",
    "    x += 1.0/(i**2)\n",
    "t1 = time.time()\n",
    "print(\"x = %f\"%x)\n",
    "print(\"time=%f\"%(t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de9377c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://macbook-pro-10:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>COSC608_spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff67cefcd60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17621393",
   "metadata": {},
   "source": [
    "## Working with DataFrames\n",
    "Spark has shifted paradigm from RDD to DataFrames, similar to R or Python/Pandas dataframes\n",
    "\n",
    "### Self-created DataFrames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "29e79e4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"Java\", 1200), (\"Python\", 10000), (\"Scala\", 3000)]\n",
    "df = spark.createDataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c94f6360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'pyspark.sql.types.Row'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = df.collect()\n",
    "print(type(a), type(a[0]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f9b2597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|    _1|   _2|\n",
      "+------+-----+\n",
      "|  Java| 1200|\n",
      "|Python|10000|\n",
      "| Scala| 3000|\n",
      "+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e85d0ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tipsFile = \"/Users/ahenschel/Dropbox/COSC101/Exercises/tips.csv\"\n",
    "tips = spark.read.csv(tipsFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "bb8e44d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|       _c0| _c1|   _c2|   _c3|_c4|   _c5| _c6|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7f0c2c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[total_bill: string, tip: string, sex: string, smoker: string, day: string, time: string, size: string]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips = spark.read.option('header', 'true').csv(tipsFile)\n",
    "tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "2da2e3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e13df5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "863870e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- total_bill: string (nullable = true)\n",
      " |-- tip: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- smoker: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- size: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "889c5e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[total_bill: double, tip: double, sex: string, smoker: string, day: string, time: string, size: int]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips = spark.read.option('header', 'true').csv(tipsFile, inferSchema=True)\n",
    "tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "877697d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- total_bill: double (nullable = true)\n",
      " |-- tip: double (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- smoker: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "38804785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(total_bill=16.99, tip=1.01, sex='Female', smoker='No', day='Sun', time='Dinner', size=2),\n",
       " Row(total_bill=10.34, tip=1.66, sex='Male', smoker='No', day='Sun', time='Dinner', size=3),\n",
       " Row(total_bill=21.01, tip=3.5, sex='Male', smoker='No', day='Sun', time='Dinner', size=3)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad68aa5c",
   "metadata": {},
   "source": [
    "### Column selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "89f7a7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "| tip|   sex|\n",
      "+----+------+\n",
      "|1.01|Female|\n",
      "|1.66|  Male|\n",
      "| 3.5|  Male|\n",
      "|3.31|  Male|\n",
      "+----+------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.select(['tip', 'sex']).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "509b70f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('total_bill', 'double'),\n",
       " ('tip', 'double'),\n",
       " ('sex', 'string'),\n",
       " ('smoker', 'string'),\n",
       " ('day', 'string'),\n",
       " ('time', 'string'),\n",
       " ('size', 'int')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "64ffdfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------+------+----+------+------------------+\n",
      "|summary|        total_bill|               tip|   sex|smoker| day|  time|              size|\n",
      "+-------+------------------+------------------+------+------+----+------+------------------+\n",
      "|  count|               244|               244|   244|   244| 244|   244|               244|\n",
      "|   mean|19.785942622950824|2.9982786885245902|  null|  null|null|  null| 2.569672131147541|\n",
      "| stddev| 8.902411954856857|1.3836381890011815|  null|  null|null|  null|0.9510998047322347|\n",
      "|    min|              3.07|               1.0|Female|    No| Fri|Dinner|                 1|\n",
      "|    max|             50.81|              10.0|  Male|   Yes|Thur| Lunch|                 6|\n",
      "+-------+------------------+------------------+------+------+----+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b3b4b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "tips1 = tips.withColumn(\"tipPP\", tips['tip']/tips['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "035734be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+------------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|             tipPP|\n",
      "+----------+----+------+------+---+------+----+------------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|             0.505|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|0.5533333333333333|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|1.1666666666666667|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|             1.655|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|            0.9025|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|            1.1775|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|               1.0|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|              0.78|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|              0.98|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|             1.615|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|             0.855|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|              1.25|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|             0.785|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|              0.75|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|              1.51|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|              1.96|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|0.5566666666666666|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|1.2366666666666666|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|1.1666666666666667|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|1.1166666666666667|\n",
      "+----------+----+------+------+---+------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "faac4a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+----+------+----+------------------+\n",
      "|total_bill| tip|   sex|smoker| day|  time|size|             tipPP|\n",
      "+----------+----+------+------+----+------+----+------------------+\n",
      "|     50.81|10.0|  Male|   Yes| Sat|Dinner|   3|3.3333333333333335|\n",
      "|     24.71|5.85|  Male|    No|Thur| Lunch|   2|             2.925|\n",
      "|     23.33|5.65|  Male|   Yes| Sun|Dinner|   2|             2.825|\n",
      "|      7.25|5.15|  Male|   Yes| Sun|Dinner|   2|             2.575|\n",
      "|     22.23| 5.0|  Male|    No| Sun|Dinner|   2|               2.5|\n",
      "|     25.28| 5.0|Female|   Yes| Sat|Dinner|   2|               2.5|\n",
      "|     32.68| 5.0|  Male|   Yes|Thur| Lunch|   2|               2.5|\n",
      "|     48.33| 9.0|  Male|    No| Sat|Dinner|   4|              2.25|\n",
      "|     28.17| 6.5|Female|   Yes| Sat|Dinner|   3|2.1666666666666665|\n",
      "|      21.7| 4.3|  Male|    No| Sat|Dinner|   2|              2.15|\n",
      "|     16.32| 4.3|Female|   Yes| Fri|Dinner|   2|              2.15|\n",
      "|     25.21|4.29|  Male|   Yes| Sat|Dinner|   2|             2.145|\n",
      "|     19.81|4.19|Female|   Yes|Thur| Lunch|   2|             2.095|\n",
      "|     20.92|4.08|Female|    No| Sat|Dinner|   2|              2.04|\n",
      "|     17.92|4.08|  Male|    No| Sat|Dinner|   2|              2.04|\n",
      "|     20.49|4.06|  Male|   Yes| Sat|Dinner|   2|              2.03|\n",
      "|     27.28| 4.0|  Male|   Yes| Fri|Dinner|   2|               2.0|\n",
      "|     16.82| 4.0|  Male|   Yes| Sun|Dinner|   2|               2.0|\n",
      "|     18.28| 4.0|  Male|    No|Thur| Lunch|   2|               2.0|\n",
      "|     14.31| 4.0|Female|   Yes| Sat|Dinner|   2|               2.0|\n",
      "+----------+----+------+------+----+------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips1.sort('tipPP', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dfea1bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "323d2b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   sex|count|\n",
      "+------+-----+\n",
      "|Female|   87|\n",
      "|  Male|  157|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.groupBy('sex').count().sort('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2eea8590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "|size|          avg(tip)|\n",
      "+----+------------------+\n",
      "|   1|            1.4375|\n",
      "|   2| 2.582307692307693|\n",
      "|   3| 3.393157894736842|\n",
      "|   5|4.0280000000000005|\n",
      "|   4| 4.135405405405407|\n",
      "|   6|             5.225|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.groupBy('size').mean(\"tip\").orderBy(\"avg(tip)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f47d63",
   "metadata": {},
   "source": [
    "### Exercise - Aggregate functions\n",
    "[https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/]\n",
    "Calculate, whether men or women tip more, per person and percentage wise. To this end, calculate for each table the percentage tip (multiply by 100 for better readability) and divide that by size. Add a new column tipPctPP for this. Then calculate the average of this column for men and for women. Use the groupBy method and confirm with the filter method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5e64e1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+------------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|          tipPctPP|\n",
      "+----------+----+------+------+---+------+----+------------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|2.9723366686286052|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|5.3513862024500325|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3| 5.552911312073615|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|  6.98902027027027|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4| 3.670191134607564|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4| 4.655990510083037|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|11.402508551881414|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|2.9017857142857144|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2| 6.515957446808511|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|10.926928281461436|\n",
      "+----------+----+------+------+---+------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips2 = tips.withColumn(\"tipPctPP\", (100*tips['tip']/tips['total_bill'])/tips['size'])\n",
    "tips2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "574eb234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+------------------+------------------+-----------------+\n",
      "|   sex|   avg(total_bill)|          avg(tip)|         avg(size)|    avg(tipPctPP)|\n",
      "+------+------------------+------------------+------------------+-----------------+\n",
      "|Female|18.056896551724137| 2.833448275862069|2.4597701149425286|7.739127915090128|\n",
      "|  Male|20.744076433121034|3.0896178343949052|2.6305732484076434|6.742178656313673|\n",
      "+------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips2.groupBy(\"sex\").mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e7339355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    avg(tipPctPP)|\n",
      "+-----------------+\n",
      "|7.739127915090128|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips2.filter(tips2.sex=='Female').select(mean('tipPctPP')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ddc76c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(tip)=2.833448275862069, avg(tipPctPP)=7.739127915090128)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tips2.filter(tips2.sex=='Female').select(mean('tip'), mean('tipPctPP')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ab632fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    avg(tipPctPP)|\n",
      "+-----------------+\n",
      "|6.742178656313673|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips2.filter(tips2.sex=='Male').select(mean('tipPctPP')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdce7b0",
   "metadata": {},
   "source": [
    "## Group-By aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce46cc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- bonus: long (nullable = true)\n",
      "\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|James        |Sales     |NY   |90000 |34 |10000|\n",
      "|Michael      |Sales     |NY   |86000 |56 |20000|\n",
      "|Robert       |Sales     |CA   |81000 |30 |23000|\n",
      "|Maria        |Finance   |CA   |90000 |24 |23000|\n",
      "|Raman        |Finance   |CA   |99000 |40 |24000|\n",
      "|Scott        |Finance   |NY   |83000 |36 |19000|\n",
      "|Jen          |Finance   |NY   |79000 |53 |15000|\n",
      "|Jeff         |Marketing |CA   |80000 |25 |18000|\n",
      "|Kumar        |Marketing |NY   |91000 |50 |21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n",
    "schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(data=simpleData, schema=schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dec1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://sparkbyexamples.com/pyspark-tutorial/\n",
    "df1 = df.groupBy(\"department\").sum(\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd637c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|department|sum(salary)|\n",
      "+----------+-----------+\n",
      "|Sales     |257000     |\n",
      "|Finance   |351000     |\n",
      "|Marketing |171000     |\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1afb3f",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "1. Group by state and create a count for each state\n",
    "2. Group by state and calculate the respective average salaries\n",
    "3. Group by both department and state and calculate the mean bonus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b4dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6eb09a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+--------+----------+\n",
      "|state|avg(salary)|avg(age)|avg(bonus)|\n",
      "+-----+-----------+--------+----------+\n",
      "|CA   |87500.0    |29.75   |22000.0   |\n",
      "|NY   |85800.0    |45.8    |17000.0   |\n",
      "+-----+-----------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"state\").mean().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8b886766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|state|count|\n",
      "+-----+-----+\n",
      "|CA   |4    |\n",
      "|NY   |5    |\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"state\").count().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b9e573ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[department: string, max(salary): bigint]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy(\"department\").max(\"salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30b682d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+\n",
      "|state|department|avg(salary)|\n",
      "+-----+----------+-----------+\n",
      "|CA   |Sales     |81000.0    |\n",
      "|CA   |Marketing |80000.0    |\n",
      "|NY   |Sales     |88000.0    |\n",
      "|CA   |Finance   |94500.0    |\n",
      "|NY   |Finance   |81000.0    |\n",
      "|NY   |Marketing |91000.0    |\n",
      "+-----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"state\", \"department\").mean(\"salary\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905741b",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "Note the Array types. Identify columns that are of ArrayType. Note that this is different to most SQL databases like MySQL or SQLite. It's a good idea to use column names that qualify as\n",
    "Caveat: don't use column names that are methods/attributes of a PySpark DataFrame object, like \"mean\" or \"sum\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75d46ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n",
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
      "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType,StructField \n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType\n",
    "\n",
    "\n",
    "data = [\n",
    "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    " ]\n",
    "        \n",
    "schema = StructType([\n",
    "     StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "     ])),\n",
    "     StructField('languages', ArrayType(StringType()), True),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    " ])\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(df.state=\"OH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0954e625",
   "metadata": {},
   "source": [
    "## Working with Time Series\n",
    "Time Series usually contain a time stamp, often as index, which can then be used for quickly selecting individual times or periods of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "46ab0ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|               input|\n",
      "+---+--------------------+\n",
      "|  1|2019-07-01 12:01:...|\n",
      "|  2|2019-06-24 12:01:...|\n",
      "|  3|2019-11-16 16:44:...|\n",
      "|  4|2019-11-16 16:50:...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates = [(\"1\",\"2019-07-01 12:01:19.111\"),\n",
    "    (\"2\",\"2019-06-24 12:01:19.222\"),\n",
    "    (\"3\",\"2019-11-16 16:44:55.406\"),\n",
    "    (\"4\",\"2019-11-16 16:50:59.406\")\n",
    "    ]\n",
    "ts = spark.createDataFrame(data=dates, schema=[\"id\",\"input\"])\n",
    "ts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "86d6ba7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: string, input: string]\n"
     ]
    }
   ],
   "source": [
    "print(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f7a706",
   "metadata": {},
   "source": [
    "### Converting strings to dates\n",
    "Note that we can't easily select for example all entries from 2019 or from morning hours or from november (etc.), if the data type is still a string. We need to convert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "367abc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "current_timestamp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c2235171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'current_timestamp()'>"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9644f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_timestamp?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "c2ba22cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = ts.withColumn('start_time',to_timestamp(col('input')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f523107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|               input|          start_time|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|2019-07-01 12:01:...|2019-07-01 12:01:...|\n",
      "|  2|2019-06-24 12:01:...|2019-06-24 12:01:...|\n",
      "|  3|2019-11-16 16:44:...|2019-11-16 16:44:...|\n",
      "|  4|2019-11-16 16:50:...|2019-11-16 16:50:...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "50d827a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: string, input: string, start_time: timestamp]\n"
     ]
    }
   ],
   "source": [
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "9f6f5a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|               input|          start_time|            end_time|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  1|2019-07-01 12:01:...|2019-07-01 12:01:...|2022-11-04 21:19:...|\n",
      "|  2|2019-06-24 12:01:...|2019-06-24 12:01:...|2022-11-04 21:19:...|\n",
      "|  3|2019-11-16 16:44:...|2019-11-16 16:44:...|2022-11-04 21:19:...|\n",
      "|  4|2019-11-16 16:50:...|2019-11-16 16:50:...|2022-11-04 21:19:...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts = ts.withColumn('end_time', current_timestamp())\n",
    "ts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2ec0a802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+-------------+\n",
      "| id|               input|          start_time|            end_time|DiffInSeconds|\n",
      "+---+--------------------+--------------------+--------------------+-------------+\n",
      "|  1|2019-07-01 12:01:...|2019-07-01 12:01:...|2022-11-04 21:20:...|    105614337|\n",
      "|  2|2019-06-24 12:01:...|2019-06-24 12:01:...|2022-11-04 21:20:...|    106219137|\n",
      "|  3|2019-11-16 16:44:...|2019-11-16 16:44:...|2022-11-04 21:20:...|     93674121|\n",
      "|  4|2019-11-16 16:50:...|2019-11-16 16:50:...|2022-11-04 21:20:...|     93673757|\n",
      "+---+--------------------+--------------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Note, this is a rather complex calculation, not easy to do by yourself\n",
    "ts = ts.withColumn('DiffInSeconds',col(\"end_time\").cast(\"long\") - col('start_time').cast(\"long\"))\n",
    "ts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d29ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c17c2e1c",
   "metadata": {},
   "source": [
    "## Joining Tables\n",
    "Again, a complex operation, also in terms of $O$-notation. It is also refered to the cross-product. The join benefits from indexing the columns that the join is performed on.\n",
    "\n",
    "### Types of Joins\n",
    "\n",
    "Join String\t|Equivalent SQL Join\n",
    "---|---\n",
    "inner|\tINNER JOIN\n",
    "outer, full, fullouter, full_outer|\tFULL OUTER JOIN\n",
    "left, leftouter, left_outer\t|LEFT JOIN\n",
    "right, rightouter, right_outer |RIGHT JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c6ab247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15192ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e8ccfe",
   "metadata": {},
   "source": [
    "Notice, how fast the next command is. Explain, why!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c69fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "jempDF = empDF.join(deptDF,empDF.emp_dept_id == deptDF.dept_id, \"inner\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f32f03",
   "metadata": {},
   "source": [
    "Many operations could still be performed on the joined table, or rather, on the table \"to be joined\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "090995a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|emp_id|    name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "|     3|Williams|              1|       2010|         10|     M|  1000|  Finance|     10|\n",
      "|     4|   Jones|              2|       2005|         10|     F|  2000|  Finance|     10|\n",
      "|     2|    Rose|              1|       2010|         20|     M|  4000|Marketing|     20|\n",
      "|     5|   Brown|              2|       2010|         40|      |    -1|       IT|     40|\n",
      "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jempDF.filter(jempDF.name!='Smith').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be703573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ahenschel/Dropbox/COSC608/Code\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a9bd79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
